{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Batch Update vs. Event-Driven"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With Neurogrid, the system architecture is event-driven: the neurons do their computations independently and continuously and only interact with the digital world when they send or receive spikes. Synaptic connections are only processed when there are spikes occurring. Using probability trees makes sense in this paradigm because only $O(logn)$ work is done for each spike issued by the neurons.\n",
      "\n",
      "With Nengo, the computations are done in a batch-update mode: every ms of simulation time, the neurons update their state based on their present input, compute whether or not they have spiked, and matrix-vector multiplies are performed to compute the next input (I might have the specific order of things wrong here, but it's something like this. You could design custom hardware to support this kind of batch-update as well.\n",
      "\n",
      "If you want to compare the efficiency of implementing these two approaches, consider the following equation:\n",
      "\n",
      "$Work_{event} = O(f_{spk} * n * log(n))$\n",
      "\n",
      "$Work_{batch} = O(f_{batch} * n * d)$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# this cell is all about the batch update/rate architecture\n",
      "\n",
      "f_batch = 1000 # Hz, 1 ms update rate\n",
      "\n",
      "k = 50 # neurons per dimension\n",
      "D = 512 # total dims\n",
      "D_sub = 16 # dims per pool\n",
      "\n",
      "N_buf = k*D*3 # number of neurons in \"buffer\" pools\n",
      "N_mult = k*4*D*2 # number of neurons in \"mutliplier\" pools\n",
      "\n",
      "n_xform = 6\n",
      "\n",
      "# neurons * (encode_dims + decode_dims) * (3 buffer pools) * rate\n",
      "mult_rate = (N_buf * D_sub*2 + N_mult * 3 + n_xform * D**2) * f_batch\n",
      "print \"multiply rate = \", mult_rate, \" multplies/s\"\n",
      "print \"              = \", mult_rate/1e9, \" G multiplies/s\\n\"\n",
      "\n",
      "# 8 bits read per multiply\n",
      "mem_access_rate = mult_rate * 8\n",
      "print \"mem access rate = \", mem_access_rate, \" bit/s\"\n",
      "print \"                = \", mem_access_rate/1e9, \" Gbit/s\"\n",
      "\n",
      "b = 8 # bit width of decoded value\n",
      "a = 32 # address length\n",
      "\n",
      "N_buf_pools = D/D_sub # 32 16-D ensembles combine for the 512D buffer\n",
      "FO = D*2/D_sub # each 16D guy fans out to 64 2x16 transform matrix chunks\n",
      "\n",
      "buf_2_xform = 2*N_buf_pools*FO*(D_sub*b+a) # dimensions sent between input buffer decoder and 1st xform (DFT)\n",
      "xform_2_mult = 2*D*(2*b+a) # dimensions sent between 2nd xform and multiplier encoder\n",
      "mult_2_xform = 2*D*(b+a) # between multiplier decoder and 2nd xform (invDFT)\n",
      "xform_2_buf = D*(D_sub*b+a) # between 2nd xform and output buffer encoder\n",
      "\n",
      "total_traffic = f_batch * (buf_2_xform + \n",
      "                           xform_2_mult + \n",
      "                           mult_2_xform + \n",
      "                           xform_2_buf)\n",
      "print \"total traffic = \", total_traffic, \" bit/s\"\n",
      "print \"              = \", total_traffic/1e9, \" Gbit/s\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "multiply rate =  4644864000  multplies/s\n",
        "              =  4.644864  G multiplies/s\n",
        "\n",
        "mem access rate =  37158912000  bit/s\n",
        "                =  37.158912  Gbit/s\n",
        "total traffic =  827392000  bit/s\n",
        "              =  0.827392  Gbit/s\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# this cell is about the event-driven/spike-based architecture\n",
      "\n",
      "fmax = 1000 # I sure hope this is reasonable...\n",
      "cconv_fmax_rate = 1.7e6 # pulled from my traffic simulation, rate of fmaxes of spikes being put into network\n",
      "\n",
      "total_traffic = fmax * cconv_fmax_rate*a\n",
      "print \"total traffic = \", total_traffic, \" bit/s\"\n",
      "print \"              = \", total_traffic/1e9, \" Gbit/s\\n\"\n",
      "\n",
      "h = 64 # header size\n",
      "w = 64 # size of 8ary word\n",
      "stage_rate = cconv_fmax_rate * fmax / 3 # stages are decode, transform, encode: assumes the traffic is roughly the same in all three\n",
      "# you access a header at each stage, but different numbers of ptree words depending on size of target\n",
      "mem_access_rate = cconv_fmax_rate * h + stage_rate * (2*w + w + 4*w)\n",
      "\n",
      "print \"mem access rate = \", mem_access_rate, \" bit/s\"\n",
      "print \"                = \", mem_access_rate/1e9, \" Gbit/s\\n\"\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "total traffic =  54400000000.0  bit/s\n",
        "              =  54.4  Gbit/s\n",
        "\n",
        "mem access rate =  2.53975466667e+11  bit/s\n",
        "                =  253.975466667  Gbit/s\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}