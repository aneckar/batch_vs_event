{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Batch Update vs. Event-Driven"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With Neurogrid, the system architecture is event-driven: the neurons do their computations independently and continuously and only interact with the digital world when they send or receive spikes. Synaptic connections are only processed when there are spikes occurring. Using probability trees makes sense in this paradigm because only $O(logn)$ work is done for each spike issued by the neurons. If you try to do event-driven with discrete weights or the \"classic\" approach to probabilistic weights (which also requires storing an NxN weight matrix), you'll need to perform an $O(N)$ operation with each spike.\n",
      "\n",
      "With Nengo, the computations are done in a batch-update mode: every ms of simulation time, the neurons update their state based on their present input, compute whether or not they have spiked, and matrix-vector multiplies are performed to compute the next input (I might have the specific order of things wrong, but it's something like this). In this computation style, the decode-transform-encode steps are essentially all performed as matrix-vector multiplies. \n",
      "\n",
      "To compare the efficiency of implementing these two approaches, consider the following example:\n",
      "\n",
      "There are two layers of $P$ pools, each with $D_{sub}$ dimensions, where $D = D_{sub}*P$. The two arrays of pools are fully-connected with eachother via $D_{sub}$x$D_{sub}$ transform matrices (there are $(D/D_{sub})^2$ of these). Abstractly, the left layer of pools decodes $D$ dimensions, which is transformed by a $D$x$D$ matrix, then encoded by the right pool."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Here is a drawing:\n",
      "\n",
      "[]--D--[            ]--E--[]\n",
      "       [  crossbar  ]\n",
      "[]--D--[    of 9    ]--E--[]\n",
      "       [ transforms ]\n",
      "[]--D--[            ]--E--[]\n",
      "\n",
      "Here, P is 3\n",
      "'''\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 63,
       "text": [
        "'\\nHere is a drawing:\\n\\n[]--D--[            ]--E--[]\\n       [  crossbar  ]\\n[]--D--[    of 9    ]--E--[]\\n       [ transforms ]\\n[]--D--[            ]--E--[]\\n\\nHere, P is 3\\n'"
       ]
      }
     ],
     "prompt_number": 63
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "Let $n = k*D$, the number of neurons in an entire layer, where $k$ is a constant, typically 50. $f_{spk}$ is the firing rate of a single neuron.\n",
      "\n",
      "\\begin{align}\n",
      "R_{event} &= O(f_{spk} * n * H * log(n) * \\sqrt{D_{sub}}) \\\\\n",
      "&= O(f_{spk} * k * D * H * log(k * D) * \\sqrt{D_{sub}}) \\\\\n",
      "&= O(f_{spk} * k * D * H * (log(k) + log(D)) * \\sqrt{D_{sub}}) \\\\\n",
      "\\end{align}\n",
      "\n",
      "Here, $f_spk * n$ gives you the firing rate of all the neurons together. Each time a neuron fires, we have to walk a tree. The height of this tree is $log(n)$ (we can kind of ignore the fact that we're going decode-transform-encode, the important thing is how many targets we have: in this case, any neuron in the second layer). We also include a $\\sqrt{D_sub}$ traffic penalty. This is the \"extra spikes\" phenomenon. Basically, as you increase dimensions, you have to increase traffic because of how the spikes are split among dimensions: a neuron with decoder $x$ can wind up hitting a target neuron with encoder $y$ where $x \\perp y$. This manifests as noise in the receiving layer and saps from the signal. In order to overcome this, you need to increase the base traffic injection rate by the square root of the dimensionality.\n",
      "\n",
      "This is the rate of work that needs to be done for the event-driven, probability-tree-walking architecture. $H$ is the average \"how many\" value. \n",
      "\n",
      "\\begin{align}\n",
      "R_{batch} &= O(f_{batch} * (n * D_{sub} * 2 + D^2)) \\\\ \n",
      "&= O(f_{batch} * (k * P * D_{sub}^2 * 2 + D^2)) \\\\\n",
      "&= O(f_{batch} * (k * P * D_{sub}^2 * 2 + D_{sub}^2*P^2)) \\\\\n",
      "\\end{align}\n",
      "\n",
      "Each batch update, you compute a $D_{sub}$x$n$ matrix-vector product to get the decode from the first layer, a $D$x$D$ matrix-vector product to do the transform (effectively, anyway, it doesn't really matter that it's split up into smaller pieces in terms of the amount of work done) and a $n$x$D_{sub}$ matrix-vector product to get the encode to the second pool.\n",
      "\n",
      "Looking at these two, it's hard to tell which is superior. Let's plug in a couple sets of values for these equations:\n",
      "\n",
      "The following constants don't change much\n",
      "\\begin{align}\n",
      "f_{spk} &= 50 \\\\\n",
      "f_{batch} &= 1000 \\\\\n",
      "k &= 50 \\\\\n",
      "H &= 10 \\\\\n",
      "\\end{align}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Network Parameter Set 1:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align}\n",
      "D &= 512 \\\\\n",
      "D_{sub} &= 16 \\\\\n",
      "P &= 32 \\\\\n",
      "\\end{align}\n",
      "\n",
      "This is bit like the circular convolution:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "def CompareArch(f_spk, f_batch, k, H, D, D_sub, P):\n",
      "    R_event = f_spk * k * D * H * (np.log2(k) + np.log2(D)) * np.sqrt(D_sub)\n",
      "    print \"R_event = f_spk * k * D * H * (np.log2(k) + np.log2(D)) * np.sqrt(D_sub)\"\n",
      "    print \"        = \", f_spk * k * D * H, \" * (\", np.log2(k), \" + \", np.log2(D), \") * \", np.sqrt(D_sub)\n",
      "    print \"        = \", R_event/10.**6, \" M\"\n",
      "    \n",
      "    R_batch = f_batch * (k * P * D_sub**2 * 2 + D_sub**2*P**2)\n",
      "    print \"R_batch = f_batch * (k * P * D_sub**2 * 2 + D_sub**2*P**2)\"\n",
      "    print \"        = \", f_batch, \" * (\", k * P * D_sub**2 * 2, \" + \", D_sub**2*P**2, \")\"\n",
      "    print \"        = \", R_batch/10.**6, \" M\"\n",
      "\n",
      "f_spk = 50\n",
      "f_batch = 1000\n",
      "k = 50\n",
      "H = 10"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "D = 512\n",
      "D_sub = 16\n",
      "P = D/D_sub\n",
      "CompareArch(f_spk, f_batch, k, H, D, D_sub, P)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "R_event = f_spk * k * D * H * (np.log2(k) + np.log2(D)) * np.sqrt(D_sub)\n",
        "        =  12800000  * ( 5.64385618977  +  9.0 ) *  4.0\n",
        "        =  749.765436916  M\n",
        "R_batch = f_batch * (k * P * D_sub**2 * 2 + D_sub**2*P**2)\n",
        "        =  1000  * ( 819200  +  262144 )\n",
        "        =  1081.344  M\n"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Network Parameter Set 2:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "\\begin{align}\n",
      "D &= 512 \\\\\n",
      "D_{sub} &= 64 \\\\\n",
      "P &= 8 \\\\\n",
      "\\end{align}\n",
      "\n",
      "This is the same thing, but with higher-D ensembles. (Maybe a moot example, this would take a huge amount of memory on-chip). Anyway, this shows Kwabena's point that the prob tree might do better with \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "D = 512\n",
      "D_sub = 64\n",
      "P = D/D_sub\n",
      "CompareArch(f_spk, f_batch, k, H, D, D_sub, P)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "R_event = f_spk * k * D * H * (np.log2(k) + np.log2(D)) * np.sqrt(D_sub)\n",
        "        =  12800000  * ( 5.64385618977  +  9.0 ) *  8.0\n",
        "        =  1499.53087383  M\n",
        "R_batch = f_batch * (k * P * D_sub**2 * 2 + D_sub**2*P**2)\n",
        "        =  1000  * ( 3276800  +  262144 )\n",
        "        =  3538.944  M\n"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Network Parameter Set 3:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\\begin{align}\n",
      "D &= 3 \\\\\n",
      "D_{sub} &= 3 \\\\\n",
      "P &= 1 \\\\\n",
      "\\end{align}\n",
      "\n",
      "This is just one 3D pool talking to another through a transform matrix"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "D = 3\n",
      "D_sub = 3\n",
      "P = D/D_sub\n",
      "CompareArch(f_spk, f_batch, k, H, D, D_sub, P)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "R_event = f_spk * k * D * H * (np.log2(k) + np.log2(D)) * np.sqrt(D_sub)\n",
        "        =  75000  * ( 5.64385618977  +  1.58496250072 ) *  1.73205080757\n",
        "        =  0.939051093798  M\n",
        "R_batch = f_batch * (k * P * D_sub**2 * 2 + D_sub**2*P**2)\n",
        "        =  1000  * ( 900  +  9 )\n",
        "        =  0.909  M\n"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Take-aways From the Math"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our conclusion at this point should be no conclusion: there are no clear scaling arguments we can make either way for the network sizes we're interested in. At this point, the constants really start to matter"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Concrete Example: Circular Convolution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It's easy to do good back-of-the-envelope calculations to figure out how many operations you need to do for batch mode, so here we do that for the circular convolution. Doing back-of-the-envelope is kind of tough to be confident about for event-driven, but I have simulation results that I am confident about that I can use to make a comparison, that's done further down."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# this cell is all about the batch update/rate architecture\n",
      "\n",
      "f_batch = 1000 # Hz, 1 ms update rate\n",
      "\n",
      "k = 50 # neurons per dimension\n",
      "D = 512 # total dims\n",
      "D_sub = 16 # dims per pool\n",
      "\n",
      "N_buf = k*D*3 # number of neurons in \"buffer\" pools\n",
      "N_mult = k*4*D*2 # number of neurons in \"mutliplier\" pools\n",
      "\n",
      "n_xform_big = 4 # number of 512x1024 transform matrices\n",
      "# buffers A and B each connect to the real and imaginary multipliers, each of which have 1024 input dimensions\n",
      "# (2048 total multiplier inputs, 1024 outputs), so there are 4 transforms on the input side\n",
      "# A and B each only hit half the dimensions in the 1024 ensembles, so if you had special\n",
      "# hardware to handle a 2-stride, you could cut this in half\n",
      "\n",
      "# The real and imaginary multipliers feed both feed pool C\n",
      "n_xform_small = 2\n",
      "\n",
      "# (neurons * (encode_dims + decode_dims) * (3 buffer pools) + xform entries) * rate\n",
      "mult_rate = (N_buf * D_sub*2 + N_mult * 3 + n_xform_big * 2*D**2 + n_xform_small * D**2) * f_batch\n",
      "print \"multiply rate = \", mult_rate, \" multplies/s\"\n",
      "print \"              = \", mult_rate/1e9, \" G multiplies/s\\n\"\n",
      "\n",
      "# 8 bits read per multiply\n",
      "mem_access_rate = mult_rate * 8\n",
      "print \"mem access rate = \", mem_access_rate, \" bit/s\"\n",
      "print \"                = \", mem_access_rate/1e9, \" Gbit/s\\n\"\n",
      "\n",
      "b = 8 # bit width of decoded value\n",
      "a = 32 # address length\n",
      "\n",
      "# I'm assuming that I've chunked my big transforms into 1024x16 pieces\n",
      "# The traffic computation depends strongly on what your architecture allows\n",
      "# This warrants further explanation of what I'm envisioning for the potential\n",
      "# batch mode architecture (maybe I'll do another  writeup for this) \n",
      "# but without going into extreme detail: you're limited by the maximum xform \n",
      "# matrix size as denoted by the size of the local memories at each core: \n",
      "# splitting up the matrices into smaller chunks essentially forces\n",
      "# you to either broadcast or aggregate decoded values across the network\n",
      "# at some point and this leads to increased network traffic\n",
      "\n",
      "P = D/D_sub # 32 16-D ensembles combine for the 512D buffer\n",
      "\n",
      "buf_2_xform = 2*2*P*(D_sub*b+a) # dimensions sent between input buffer decoder and 1st xform (DFT)\n",
      "xform_2_mult = 2*2*P*D*(2*b+a) # dimensions sent between 1st xform and multiplier encoder\n",
      "mult_2_xform = 2*D*(b+a) # between multiplier decoder and 2nd xform (invDFT)\n",
      "xform_2_buf = 2*P*D*(D_sub*b+a) # between 2nd xform and output buffer encoder\n",
      "\n",
      "total_traffic = f_batch * (buf_2_xform + \n",
      "                           xform_2_mult + \n",
      "                           mult_2_xform + \n",
      "                           xform_2_buf)\n",
      "print \"stage traffic\"\n",
      "print \"  1:\", buf_2_xform\n",
      "print \"  2:\", xform_2_mult\n",
      "print \"  3:\", mult_2_xform\n",
      "print \"  4:\", xform_2_buf\n",
      "print \"total traffic = \", total_traffic, \" bit/s\"\n",
      "print \"              = \", total_traffic/1e9, \" Gbit/s\\n\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "multiply rate =  5693440000  multplies/s\n",
        "              =  5.69344  G multiplies/s\n",
        "\n",
        "mem access rate =  45547520000  bit/s\n",
        "                =  45.54752  Gbit/s\n",
        "\n",
        "stage traffic\n",
        "  1: 20480\n",
        "  2: 3145728\n",
        "  3: 40960\n",
        "  4: 5242880\n",
        "total traffic =  8450048000  bit/s\n",
        "              =  8.450048  Gbit/s\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# this cell is about the event-driven/spike-based architecture\n",
      "\n",
      "fmax = 1000 # I sure hope this is reasonable...\n",
      "cconv_fmax_rate = 1.7e6 # pulled from my traffic simulation, rate of fmaxes of spikes being put into network\n",
      "\n",
      "total_traffic = fmax * cconv_fmax_rate*a\n",
      "print \"total traffic = \", total_traffic, \" bit/s\"\n",
      "print \"              = \", total_traffic/1e9, \" Gbit/s\\n\"\n",
      "\n",
      "h = 64 # header size\n",
      "w = 64 # size of 8ary word\n",
      "stage_rate = cconv_fmax_rate * fmax / 3 # stages are decode, transform, encode: assumes the traffic is roughly the same in all three\n",
      "# you access a header at each stage, but different numbers of ptree words depending on size of target\n",
      "mem_access_rate = cconv_fmax_rate * h + stage_rate * (2*w + w + 4*w)\n",
      "\n",
      "print \"mem access rate = \", mem_access_rate, \" bit/s\"\n",
      "print \"                = \", mem_access_rate/1e9, \" Gbit/s\\n\"\n",
      "\n",
      "LFSR_cmp_rate = stage_rate * (6 + 2 + 12)\n",
      "\n",
      "print \"lfsr/comparison rate = \", LFSR_cmp_rate, \" op/s\"\n",
      "print \"                     = \", LFSR_cmp_rate/1e9, \" G op/s\\n\"\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "total traffic =  54400000000.0  bit/s\n",
        "              =  54.4  Gbit/s\n",
        "\n",
        "mem access rate =  2.53975466667e+11  bit/s\n",
        "                =  253.975466667  Gbit/s\n",
        "\n",
        "lfsr/comparison rate =  11333333333.3  op/s\n",
        "                     =  11.3333333333  G op/s\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 61
    }
   ],
   "metadata": {}
  }
 ]
}